{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ce1833b-aadc-4fb7-923d-2dbbc60c082d",
   "metadata": {},
   "source": [
    "1. Choose the appropriate model (Cell 1)\n",
    "2. Choose the appropriate dataset and the final file where the word embeddings must be saved (Cell 2)\n",
    "3. Run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2697a2a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-13T12:47:31.072316Z",
     "start_time": "2021-11-13T12:46:59.687059Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27533bc2d1d342fb98e71ce20e1f815d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eb5646653e7406090459f786d27a318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7841847cbdfe4edeab87390fc2d028e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0392b0dd93f54addb254d90ed1e9afe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93ca6b8ddac647c5a7d396e25b248e72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62d218da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-13T12:47:31.077407Z",
     "start_time": "2021-11-13T12:47:31.074818Z"
    }
   },
   "outputs": [],
   "source": [
    "DATASET_DIR = '../all_datasets_10_shot/hwu_ood'\n",
    "DEST = 'BERT_Base_hwu.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8567cb43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-13T12:47:31.087751Z",
     "start_time": "2021-11-13T12:47:31.078710Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenizeSimple(sent, max_token_size=100000):\n",
    "    ret = sent.lower()\n",
    "    ret = ret.replace(\"\\\"\", \" \\\" \")\n",
    "    ret = ret.replace(\",\", \" , \")\n",
    "    ret = ret.replace(\".\", \" . \")\n",
    "    ret = ret.replace(\"(\", \" ( \")\n",
    "    ret = ret.replace(\")\", \" ) \")\n",
    "    ret = ret.replace(\"/\", \" / \")\n",
    "    ret = ret.replace(\"?\", \" ? \")\n",
    "    ret = ret.replace(\"!\", \" ! \")\n",
    "    ret = ret.replace(\"n't\", \" n't \")\n",
    "    ret = ret.replace(\"'ve \", \" 've \")\n",
    "    ret = ret.replace(\"'ll \", \" 'll \")\n",
    "    ret = ret.replace(\"'re \", \" 're \")\n",
    "    ret = ret.replace(\"'s \", \" 's \")\n",
    "    ret = ret.replace(\"'m \", \" 'm \")\n",
    "    ret = ret.replace(\"'d \", \" 'd \")\n",
    "    ret = re.sub(\" +\", ' ', ret)\n",
    "    ret = ret.strip()\n",
    "    ret = ret.split(' ')[:max_token_size]\n",
    "\n",
    "    while len(ret) < max_token_size:\n",
    "        ret.append(\"</s>\")\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5416d20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-13T12:48:15.421460Z",
     "start_time": "2021-11-13T12:48:15.301153Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../all_datasets_10_shot/hwu_ood/dev/seq.in\n",
      "../all_datasets_10_shot/hwu_ood/train/seq.in\n",
      "../all_datasets_10_shot/hwu_ood/test/seq.in\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(444240, 4602)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKENS = []\n",
    "for directory, _, files in os.walk(DATASET_DIR):\n",
    "    for file in files:\n",
    "        if 'seq.in' not in file: continue\n",
    "        print(os.path.join(directory, file))\n",
    "        with open(os.path.join(directory, file)) as fi:\n",
    "            for line in fi:\n",
    "                line = line.strip()\n",
    "                text = line\n",
    "                textwds = tokenizeSimple(text, 40)\n",
    "                TOKENS += textwds\n",
    "len(TOKENS), len(set(TOKENS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7103e231",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-13T12:50:38.791561Z",
     "start_time": "2021-11-13T12:48:19.097733Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4602/4602 [02:19<00:00, 32.95it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4602"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDINGS = []\n",
    "for wd in tqdm(set(TOKENS)):\n",
    "    encoded_input = tokenizer(wd, return_tensors='pt')\n",
    "    output = model(**encoded_input)\n",
    "    embedding = output['pooler_output'].detach().tolist()[0]\n",
    "    EMBEDDINGS.append(embedding)\n",
    "len(EMBEDDINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13ea97c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-13T12:50:41.211396Z",
     "start_time": "2021-11-13T12:50:38.793328Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(DEST, 'w') as f:\n",
    "    for k, v in zip(set(TOKENS), EMBEDDINGS):\n",
    "        f.write(str(k)+\"\\t\"+\" \".join([str(tt) for tt in v])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c205c47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
